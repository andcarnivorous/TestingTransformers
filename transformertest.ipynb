{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        mask = mask.to(\"cuda\")\n",
    "        return mask\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        x = self.dropout(x)\n",
    "        x = x.to(\"cuda\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead=2, num_decoder_layers=2, num_encoder_layers=2, ntokens=5000):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.nhead = nhead\n",
    "        self.d_model = d_model\n",
    "        self.ntokens = ntokens\n",
    "        self.emb = nn.Embedding(self.ntokens, self.d_model)\n",
    "        self.posenc = PositionalEncoding(d_model, 0.5)\n",
    "        self.transformer = nn.Transformer(d_model=self.d_model, nhead=self.nhead,\n",
    "                                          num_decoder_layers=self.num_decoder_layers,\n",
    "                                          num_encoder_layers=self.num_encoder_layers)\n",
    "        self.linear = nn.Linear(self.d_model, self.ntokens)\n",
    "        \n",
    "    def forward(self, x, tgt):\n",
    "        x = self.emb(x) #* math.sqrt(self.d_model)\n",
    "        tgt = self.emb(tgt)\n",
    "        #print(x.shape, tgt.shape)\n",
    "        x = self.posenc(x)\n",
    "       # print(x)\n",
    "        x = self.transformer(x, tgt, tgt_mask=generate_square_subsequent_mask(x.shape[0]), src_mask=generate_square_subsequent_mask(x.shape[0]))\n",
    "        #return x\n",
    "        return nn.functional.log_softmax(self.linear(x))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Part is from https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.datasets.language_modeling.WikiText2 object at 0x7f6dcc7b0080>\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 16\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)\n",
    "\n",
    "print(test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28785"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(TEXT.vocab.stoi)\n",
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([130419, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(d_model=200, nhead=2, num_decoder_layers=2, num_encoder_layers=2, ntokens=ntokens)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 3.0 # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(\"cuda\")\n",
    "        targets = targets.to(\"cuda\")\n",
    "        output = model(data, data)\n",
    "\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "           # print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "            #      'loss {:5.2f}'.format(\n",
    "             #       epoch, batch, len(train_data),\n",
    "              #      cur_loss))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data, data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/main/.local/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/130419 batches | loss 15.04\n",
      "| epoch   1 |   400/130419 batches | loss 31.50\n",
      "| epoch   1 |   600/130419 batches | loss 678.59\n",
      "| epoch   1 |   800/130419 batches | loss 1090.57\n",
      "| epoch   1 |  1000/130419 batches | loss 1336.41\n",
      "| epoch   1 |  1200/130419 batches | loss 3772.36\n",
      "| epoch   1 |  1400/130419 batches | loss 7021.18\n",
      "| epoch   1 |  1600/130419 batches | loss 9369.63\n",
      "| epoch   1 |  1800/130419 batches | loss 9757.67\n",
      "| epoch   1 |  2000/130419 batches | loss 14592.79\n",
      "| epoch   1 |  2200/130419 batches | loss 12107.72\n",
      "| epoch   1 |  2400/130419 batches | loss 19399.66\n",
      "| epoch   1 |  2600/130419 batches | loss 15167.50\n",
      "| epoch   1 |  2800/130419 batches | loss 17772.77\n",
      "| epoch   1 |  3000/130419 batches | loss 17753.37\n",
      "| epoch   1 |  3200/130419 batches | loss 15238.99\n",
      "| epoch   1 |  3400/130419 batches | loss 21547.16\n",
      "| epoch   1 |  3600/130419 batches | loss 26303.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 644.62s | valid loss 10.65 | valid ppl\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/130419 batches | loss 35979.22\n",
      "| epoch   2 |   400/130419 batches | loss 18807.17\n",
      "| epoch   2 |   600/130419 batches | loss 34302.71\n",
      "| epoch   2 |   800/130419 batches | loss 34774.70\n",
      "| epoch   2 |  1000/130419 batches | loss 32907.48\n",
      "| epoch   2 |  1200/130419 batches | loss 36944.48\n",
      "| epoch   2 |  1400/130419 batches | loss 45084.55\n",
      "| epoch   2 |  1600/130419 batches | loss 57020.55\n",
      "| epoch   2 |  1800/130419 batches | loss 30228.36\n",
      "| epoch   2 |  2000/130419 batches | loss 33552.07\n",
      "| epoch   2 |  2200/130419 batches | loss 40274.49\n",
      "| epoch   2 |  2400/130419 batches | loss 40881.69\n",
      "| epoch   2 |  2600/130419 batches | loss 43422.88\n",
      "| epoch   2 |  2800/130419 batches | loss 51188.12\n",
      "| epoch   2 |  3000/130419 batches | loss 55824.75\n",
      "| epoch   2 |  3200/130419 batches | loss 52014.76\n",
      "| epoch   2 |  3400/130419 batches | loss 51852.46\n",
      "| epoch   2 |  3600/130419 batches | loss 53134.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 643.86s | valid loss 10.46 | valid ppl\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/130419 batches | loss 42847.84\n",
      "| epoch   3 |   400/130419 batches | loss 58293.81\n",
      "| epoch   3 |   600/130419 batches | loss 56781.15\n",
      "| epoch   3 |   800/130419 batches | loss 66176.92\n",
      "| epoch   3 |  1000/130419 batches | loss 72639.96\n",
      "| epoch   3 |  1200/130419 batches | loss 90481.66\n",
      "| epoch   3 |  1400/130419 batches | loss 86137.59\n",
      "| epoch   3 |  1600/130419 batches | loss 60759.41\n",
      "| epoch   3 |  1800/130419 batches | loss 103522.03\n",
      "| epoch   3 |  2000/130419 batches | loss 63971.35\n",
      "| epoch   3 |  2200/130419 batches | loss 90562.48\n",
      "| epoch   3 |  2400/130419 batches | loss 83917.31\n",
      "| epoch   3 |  2600/130419 batches | loss 95314.65\n",
      "| epoch   3 |  2800/130419 batches | loss 74901.03\n",
      "| epoch   3 |  3000/130419 batches | loss 59575.47\n",
      "| epoch   3 |  3200/130419 batches | loss 74149.46\n",
      "| epoch   3 |  3400/130419 batches | loss 61938.03\n",
      "| epoch   3 |  3600/130419 batches | loss 81871.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 643.63s | valid loss 10.37 | valid ppl\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/130419 batches | loss 80345.70\n",
      "| epoch   4 |   400/130419 batches | loss 85592.48\n",
      "| epoch   4 |   600/130419 batches | loss 115674.60\n",
      "| epoch   4 |   800/130419 batches | loss 97833.01\n",
      "| epoch   4 |  1000/130419 batches | loss 113883.59\n",
      "| epoch   4 |  1200/130419 batches | loss 101918.52\n",
      "| epoch   4 |  1400/130419 batches | loss 90003.29\n",
      "| epoch   4 |  1600/130419 batches | loss 119035.83\n",
      "| epoch   4 |  1800/130419 batches | loss 138668.17\n",
      "| epoch   4 |  2000/130419 batches | loss 92431.91\n",
      "| epoch   4 |  2200/130419 batches | loss 148726.89\n",
      "| epoch   4 |  2400/130419 batches | loss 102406.91\n",
      "| epoch   4 |  2600/130419 batches | loss 153792.59\n",
      "| epoch   4 |  2800/130419 batches | loss 126351.43\n",
      "| epoch   4 |  3000/130419 batches | loss 180154.19\n",
      "| epoch   4 |  3200/130419 batches | loss 101409.44\n",
      "| epoch   4 |  3400/130419 batches | loss 102891.77\n",
      "| epoch   4 |  3600/130419 batches | loss 103575.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 643.62s | valid loss 10.35 | valid ppl\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/130419 batches | loss 108433.66\n",
      "| epoch   5 |   400/130419 batches | loss 110629.09\n",
      "| epoch   5 |   600/130419 batches | loss 115485.24\n",
      "| epoch   5 |   800/130419 batches | loss 120315.14\n",
      "| epoch   5 |  1000/130419 batches | loss 120540.56\n",
      "| epoch   5 |  1200/130419 batches | loss 125250.70\n",
      "| epoch   5 |  1400/130419 batches | loss 127483.81\n",
      "| epoch   5 |  1600/130419 batches | loss 144672.60\n",
      "| epoch   5 |  1800/130419 batches | loss 135227.31\n",
      "| epoch   5 |  2000/130419 batches | loss 143937.90\n",
      "| epoch   5 |  2200/130419 batches | loss 143828.18\n",
      "| epoch   5 |  2400/130419 batches | loss 153456.23\n",
      "| epoch   5 |  2600/130419 batches | loss 154977.64\n",
      "| epoch   5 |  2800/130419 batches | loss 166288.92\n",
      "| epoch   5 |  3000/130419 batches | loss 163802.67\n",
      "| epoch   5 |  3200/130419 batches | loss 161421.78\n",
      "| epoch   5 |  3400/130419 batches | loss 165443.54\n",
      "| epoch   5 |  3600/130419 batches | loss 164208.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 643.47s | valid loss 10.34 | valid ppl\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/130419 batches | loss 170539.17\n",
      "| epoch   6 |   400/130419 batches | loss 172028.98\n",
      "| epoch   6 |   600/130419 batches | loss 177829.94\n",
      "| epoch   6 |   800/130419 batches | loss 182188.59\n",
      "| epoch   6 |  1000/130419 batches | loss 185853.40\n",
      "| epoch   6 |  1200/130419 batches | loss 185285.49\n",
      "| epoch   6 |  1400/130419 batches | loss 184839.74\n",
      "| epoch   6 |  1600/130419 batches | loss 188977.52\n",
      "| epoch   6 |  1800/130419 batches | loss 193850.14\n",
      "| epoch   6 |  2000/130419 batches | loss 201024.34\n",
      "| epoch   6 |  2200/130419 batches | loss 198120.53\n",
      "| epoch   6 |  2400/130419 batches | loss 204646.94\n",
      "| epoch   6 |  2600/130419 batches | loss 203853.22\n",
      "| epoch   6 |  2800/130419 batches | loss 205958.41\n",
      "| epoch   6 |  3000/130419 batches | loss 218205.50\n",
      "| epoch   6 |  3200/130419 batches | loss 211959.20\n",
      "| epoch   6 |  3400/130419 batches | loss 217241.63\n",
      "| epoch   6 |  3600/130419 batches | loss 216135.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 643.58s | valid loss 10.32 | valid ppl\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/130419 batches | loss 266521.64\n",
      "| epoch   7 |   400/130419 batches | loss 236803.75\n",
      "| epoch   7 |   600/130419 batches | loss 240174.36\n",
      "| epoch   7 |   800/130419 batches | loss 244672.37\n",
      "| epoch   7 |  1000/130419 batches | loss 245396.54\n",
      "| epoch   7 |  1200/130419 batches | loss 253773.22\n",
      "| epoch   7 |  1400/130419 batches | loss 254485.26\n",
      "| epoch   7 |  1600/130419 batches | loss 256705.79\n",
      "| epoch   7 |  1800/130419 batches | loss 266956.57\n",
      "| epoch   7 |  2000/130419 batches | loss 268577.31\n",
      "| epoch   7 |  2200/130419 batches | loss 269888.91\n",
      "| epoch   7 |  2400/130419 batches | loss 285251.49\n",
      "| epoch   7 |  2600/130419 batches | loss 273750.75\n",
      "| epoch   7 |  2800/130419 batches | loss 280376.75\n",
      "| epoch   7 |  3000/130419 batches | loss 289564.73\n",
      "| epoch   7 |  3200/130419 batches | loss 292091.80\n",
      "| epoch   7 |  3400/130419 batches | loss 294683.97\n",
      "| epoch   7 |  3600/130419 batches | loss 294122.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 643.49s | valid loss 10.32 | valid ppl\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |   200/130419 batches | loss 303803.75\n",
      "| epoch   8 |   400/130419 batches | loss 309105.73\n",
      "| epoch   8 |   600/130419 batches | loss 315131.19\n",
      "| epoch   8 |   800/130419 batches | loss 314115.20\n",
      "| epoch   8 |  1000/130419 batches | loss 318878.41\n",
      "| epoch   8 |  1200/130419 batches | loss 320280.94\n",
      "| epoch   8 |  1400/130419 batches | loss 322594.71\n",
      "| epoch   8 |  1600/130419 batches | loss 328046.06\n",
      "| epoch   8 |  1800/130419 batches | loss 335126.14\n",
      "| epoch   8 |  2000/130419 batches | loss 346165.71\n",
      "| epoch   8 |  2200/130419 batches | loss 341073.02\n",
      "| epoch   8 |  2400/130419 batches | loss 355019.08\n",
      "| epoch   8 |  2600/130419 batches | loss 347089.94\n",
      "| epoch   8 |  2800/130419 batches | loss 350396.21\n",
      "| epoch   8 |  3000/130419 batches | loss 359521.67\n",
      "| epoch   8 |  3200/130419 batches | loss 360559.98\n",
      "| epoch   8 |  3400/130419 batches | loss 368141.71\n",
      "| epoch   8 |  3600/130419 batches | loss 367487.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 643.35s | valid loss 10.30 | valid ppl\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/130419 batches | loss 378478.15\n",
      "| epoch   9 |   400/130419 batches | loss 378108.80\n",
      "| epoch   9 |   600/130419 batches | loss 389565.08\n",
      "| epoch   9 |   800/130419 batches | loss 384084.83\n",
      "| epoch   9 |  1000/130419 batches | loss 387214.31\n",
      "| epoch   9 |  1200/130419 batches | loss 390539.44\n",
      "| epoch   9 |  1400/130419 batches | loss 389627.06\n",
      "| epoch   9 |  1600/130419 batches | loss 395975.98\n",
      "| epoch   9 |  1800/130419 batches | loss 397193.37\n",
      "| epoch   9 |  2000/130419 batches | loss 411149.46\n",
      "| epoch   9 |  2200/130419 batches | loss 409492.34\n",
      "| epoch   9 |  2400/130419 batches | loss 417557.48\n",
      "| epoch   9 |  2600/130419 batches | loss 412647.56\n",
      "| epoch   9 |  2800/130419 batches | loss 428316.56\n",
      "| epoch   9 |  3000/130419 batches | loss 423567.97\n",
      "| epoch   9 |  3200/130419 batches | loss 424924.23\n",
      "| epoch   9 |  3400/130419 batches | loss 428844.11\n",
      "| epoch   9 |  3600/130419 batches | loss 429666.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 643.53s | valid loss 14.53 | valid ppl\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/130419 batches | loss 441742.72\n",
      "| epoch  10 |   400/130419 batches | loss 446227.32\n",
      "| epoch  10 |   600/130419 batches | loss 453594.30\n",
      "| epoch  10 |   800/130419 batches | loss 457130.02\n",
      "| epoch  10 |  1000/130419 batches | loss 458942.03\n",
      "| epoch  10 |  1200/130419 batches | loss 466549.00\n",
      "| epoch  10 |  1400/130419 batches | loss 469322.13\n",
      "| epoch  10 |  1600/130419 batches | loss 476585.99\n",
      "| epoch  10 |  1800/130419 batches | loss 478526.55\n",
      "| epoch  10 |  2000/130419 batches | loss 492369.83\n",
      "| epoch  10 |  2200/130419 batches | loss 496371.81\n",
      "| epoch  10 |  2400/130419 batches | loss 503618.14\n",
      "| epoch  10 |  2600/130419 batches | loss 494881.13\n",
      "| epoch  10 |  2800/130419 batches | loss 496478.50\n",
      "| epoch  10 |  3000/130419 batches | loss 516449.76\n",
      "| epoch  10 |  3200/130419 batches | loss 518959.22\n",
      "| epoch  10 |  3400/130419 batches | loss 529021.67\n",
      "| epoch  10 |  3600/130419 batches | loss 540238.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 643.28s | valid loss 10.29 | valid ppl\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 10 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/main/.local/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 10.29 | test ppl 29355.64\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
