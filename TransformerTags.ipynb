{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import re\n",
    "import logging\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='POSTransformer.log',level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir(\".data/wikitext-2/wikitext-2/\")\n",
    "\n",
    "if \"wiki.train.tokens\" in datasets:\n",
    "    print(\"Removing useless symbols from datasets...\")\n",
    "    pattern = r\"[^\\w'\\.,;:\\!\\?]\"\n",
    "    for x in datasets:\n",
    "        with open(f\".data/wikitext-2/wikitext-2/{x}\", \"r\") as _f:\n",
    "            _f = _f.read()\n",
    "\n",
    "        _f = re.sub(pattern, ' ', _f)\n",
    "\n",
    "        with open(f\".data/wikitext-2/wikitext-2/{x}\", \"w\") as _w:\n",
    "            _w = _w.write(_f)\n",
    "    print(\"Cleaning finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"lr\":1.8,\"epochs\":100,\"embed_dim\":240,\"heads\":8,\"layers\":2,\"seq_len\":32,\"batch_size\":64, \"name\":\"picci\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        mask = mask.to(\"cuda\")\n",
    "        return mask\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        x = self.dropout(x)\n",
    "        x = x.to(\"cuda\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead=2, num_encoder_layers=1, ntokens=5000, output=50):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.output = output\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.nhead = nhead\n",
    "        self.d_model = d_model\n",
    "        self.ntokens = ntokens\n",
    "        self.emb = nn.Embedding(self.ntokens, self.d_model)\n",
    "        self.posenc = PositionalEncoding(d_model, 0.5)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=1024)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        self.linear = nn.Linear(self.d_model, self.output)\n",
    "        self.trans_activation = None\n",
    "        \n",
    "    def save_activations(self, epoch=0, batch=0):\n",
    "        \"\"\"\n",
    "        epoch: The running epoch, this information will be added to the files' names.\n",
    "        batch: The running batch, this information will be added to the files' names\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            \n",
    "            for x in range(len(self.transformer.layers)):\n",
    "                # save the activations of each transformer encoder layer.\n",
    "                with open(f\"/path/to/harddrive/{params['name']}-attention-{x}-{epoch}-{batch}.p\", \"wb\") as _f:\n",
    "                    pickle.dump(self.transformer.layers[x].attn_activation.cpu().detach().numpy(), _f)\n",
    "                           \n",
    "        except:\n",
    "            logging.warning(f\"Model: {params[\"name\"]} Failed to save activations for batch {batch} in epoch {epoch}! No More space left on disk!\")\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "    def forward(self, x, tgt, epoch=0, batch=0):\n",
    "        x = self.emb(x) #* math.sqrt(self.d_model)\n",
    "        tgt = self.emb(tgt)\n",
    "        x = self.posenc(x)\n",
    "        x = self.transformer(x, generate_square_subsequent_mask(x.shape[0]))\n",
    "        self.trans_activation = x\n",
    "        if not self.training:\n",
    "            self.save_activations(epoch=epoch, batch=batch)\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "TEXT = torchtext.data.Field(tokenize = \"spacy\", tokenizer_language=\"en\", \n",
    "                            init_token=None, \n",
    "                            eos_token=None, \n",
    "                            stop_words = [\"=\", \"==\", \"@\", \"@@\", \"<\", \">\", \"@-@\", \"@,@\",\"@.@\",\"@_@\", \"<eos>\", \"<sos>\", \"<pad>\"], \n",
    "                            lower=True)\n",
    "\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data\n",
    "\n",
    "batch_size = 64\n",
    "eval_batch_size = 128\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)\n",
    "\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tags = [0]*len(train_txt.examples[0].text)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for x in tqdm(range(len(train_txt.examples[0].text))):\n",
    "    tags[x] = nlp(train_txt.examples[0].text[x])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "finaltags = [0]*len(train_txt.examples[0].text)\n",
    "print(len(tags), len(finaltags))\n",
    "\n",
    "for x in tqdm(range(len(tags))):\n",
    "    for y in tags[x]:\n",
    "        finaltags[x] = y.tag_\n",
    "\n",
    "print(finaltags[:50])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(len(finaltags))\n",
    "import pickle\n",
    "with open('wikitext.train2.pickle', 'wb') as handle:\n",
    "    pickle.dump(finaltags, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wikitext.train2.pickle\", \"rb\") as h:\n",
    "    traintags = pickle.load(h)\n",
    "    \n",
    "with open(\"wikitext.valid.pickle\", \"rb\") as h:\n",
    "    valtags = pickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "itotags = {}\n",
    "tagstoi = {}\n",
    "for x in nlp.get_pipe(\"tagger\").labels:\n",
    "    itotags[counter] = x\n",
    "    tagstoi[x] = counter\n",
    "    counter += 1\n",
    "ntags = len(tagstoi.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintags = torch.Tensor([tagstoi[x] for x in traintags])\n",
    "valtags = torch.Tensor([tagstoi[x] for x in valtags])\n",
    "print(traintags.shape, valtags.shape)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traintags.shape)\n",
    "train_data.shape[0]*train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = params[\"seq_len\"]\n",
    "\n",
    "def totags(vec):\n",
    "    tagvec = [nlp(TEXT.vocab.itos[x.detach().data]) for x in vec]\n",
    "    tagvec = [y.tag_ for x in tagvec for y in x]\n",
    "    tagvec = [tagstoi[x] for x in tagvec]\n",
    "    assert len(tagvec) == len(vec)\n",
    "    return tagvec\n",
    "\n",
    "def get_tags_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    if model.training:\n",
    "        target = trainingtags[i:i+seq_len].view(-1).long()\n",
    "    else:\n",
    "        target = validationtags[i:i+seq_len].view(-1).long()\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_tags(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data\n",
    "\n",
    "trainingtags = batchify_tags(traintags, 64)\n",
    "validationtags = batchify_tags(valtags, 128)\n",
    "print(trainingtags.shape, train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi)\n",
    "\n",
    "print(trainingtags.shape)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(d_model=params[\"embed_dim\"], nhead=params[\"heads\"], num_encoder_layers=params[\"layers\"], ntokens=ntokens, output=ntags)\n",
    "model = model.to(\"cuda\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = params[\"lr\"]\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 2.0, gamma=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_tags_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device), data.to(device), batch=batch, epoch=epoch)\n",
    "        loss = criterion(output.view(-1, ntags), targets.to(device))\n",
    "        acc = targets.to(device).eq(torch.argmax(output.view(-1, ntags), dim=1)).sum().detach().cpu().numpy()\n",
    "        acc = acc / targets.shape[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 128\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | acc {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, acc))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    print(\"DATA SOURCE SIZE!\", data_source.size(0))\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_tags_batch(data_source, i)\n",
    "            output = eval_model(data.to(device), data.to(device),batch=i, epoch=epoch)\n",
    "            acc = targets.to(device).eq(torch.argmax(output.view(-1, ntags), dim=1)).sum().detach().cpu().numpy()\n",
    "            acc = acc / targets.shape[0]\n",
    "            output_flat = output.view(-1, ntags)\n",
    "            total_loss += len(data) * criterion(output_flat, targets.to(device)).item()\n",
    "    return total_loss / (len(data_source) - 1), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = params[\"epochs\"] # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "print(\"STARTING:::::::.........\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss, acc = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid acc {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, acc))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, acc = evaluate(model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test acc {:8.2f}'.format(\n",
    "    test_loss, acc))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.transformer.layers[0].attn.shape)\n",
    "print(model.transformer.layers[0].attn_preactivation.shape)\n",
    "print(model.transformer.layers[0].attn_activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = \"the president of the united states is tall\".split()\n",
    "\n",
    "\n",
    "def generate_sent(frase, model, TEXT, words=5):\n",
    "    \"\"\"\n",
    "    This function gets a list of a split string and \n",
    "    returns a new string with the next N number of words \n",
    "    predicted by the model\n",
    "    \"\"\"\n",
    "\n",
    "    frasevec = TEXT.numericalize([frase]).to(\"cuda\")\n",
    "    model.eval()\n",
    "    var = model(frasevec, frasevec)\n",
    "    most = torch.argmax(var.squeeze(1), dim=1).cpu().detach().data.numpy()\n",
    "    most = [itotags[x] for x in most]\n",
    "    print(len(frase), len(most))\n",
    "    result = [f\"{frase[x]} : {most[x]}\" for x in range(len(most))]\n",
    "    return result\n",
    "\n",
    "\n",
    "f = generate_sent(frase, model, TEXT)\n",
    "for x in f:\n",
    "    print(x)\n",
    "len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = \"the president of the united states is tall\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = [nlp(x) for x in frase]\n",
    "print(len(frase))\n",
    "frase = [y.tag_ for x in frase for y in x]\n",
    "frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = \"the president of the united states is tall\"\n",
    "a = tokenizer(frase)\n",
    "print(a)\n",
    "frase = [type(x) for x in a]\n",
    "len(frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itotags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.numericalize([nlp.tokenizer(\"this is low, ok.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.itos[12079]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\".\" in TEXT.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp.tokenizer(\"this is low, ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
